# ðŸš€ Quantization Projects Repository

A comprehensive collection of quantization techniques and implementations for optimizing AI models, developed in collaboration with Hugging Face and DeepLearning.AI.

## Project Structure

### ðŸ”¹ Quantization Fundamentals with Hugging Face
- Core concepts of model quantization using Hugging Face libraries
- Linear quantization with Quanto library
- Downcasting to BFloat16 with Transformers
- Hands-on quantization of multimodal and language models
- [View Course](https://www.deeplearning.ai/short-courses/quantization-fundamentals-with-hugging-face/)

### ðŸ”¹ Quantization in Depth
- Advanced quantization techniques beyond fundamentals
- Custom PyTorch quantizer implementation
- Per-tensor, per-channel, and per-group quantization
- Symmetric vs asymmetric quantization modes
- 2-bit weight packing/unpacking (4x compression)
- [View Course](https://www.deeplearning.ai/short-courses/quantization-in-depth/)

### ðŸ”¹ Test Suite
- Unit tests and validation scripts for quantization accuracy
- Quantization error measurement tools
- Visualization utilities for tensor analysis

## Key Features

âœ… **Comprehensive Coverage**: From basic linear quantization to advanced 2-bit packing  
âœ… **Practical Implementation**: Jupyter notebooks with hands-on code examples  
âœ… **Hugging Face Integration**: Leverage Quanto and Transformers libraries  
âœ… **Performance Optimization**: Measure and minimize quantization error  
âœ… **Production Ready**: Techniques applicable to LLMs and vision models  

## Getting Started

1. Install dependencies:
   ```bash
   pip install torch torchvision matplotlib seaborn transformers quanto
   ```

2. Open any `.ipynb` notebook in Jupyter Notebook or JupyterLab

3. Execute cells sequentially to follow along with each lesson


ðŸ”— For course enrollment and further information: [deeplearning.ai](https://www.deeplearning.ai/short-courses/)
